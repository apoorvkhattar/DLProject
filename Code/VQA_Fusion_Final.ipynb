{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sRXkeSfLTppu"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import tqdm\n",
    "import h5py\n",
    "import yaml\n",
    "import torch\n",
    "import random\n",
    "import datetime\n",
    "import torchvision\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from itertools import cycle\n",
    "from skimage import transform\n",
    "from torch.autograd import Variable\n",
    "from collections import OrderedDict\n",
    "from tensorboardX import SummaryWriter\n",
    "from mpl_toolkits.axes_grid1 import ImageGrid\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.utils import make_grid, save_image\n",
    "\n",
    "from preprocess_3 import preprocess\n",
    "from dataset import VQADataset, VQABatchSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0d6u7mXITpp6"
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "JGTNuu2wTpqE",
    "outputId": "bcb7423e-77c0-48e4-d9d5-cc455da660a2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f08391e2a70>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.seed(1)\n",
    "np.random.seed(1)\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qlx1nIE1TpqO"
   },
   "source": [
    "# Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qktwdFOxTpqQ"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/apoorv/anaconda3/envs/dlenv/lib/python3.7/site-packages/ipykernel_launcher.py:2: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "config = './config_vqa_sgd.yml'\n",
    "config = yaml.load(open(config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sA6rpDaXTpqX"
   },
   "outputs": [],
   "source": [
    "preprocess_data = True\n",
    "use_att = True\n",
    "fusion = 'block-attn'\n",
    "path_to_weights = './vqa_block_100.pth'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_SzXMnrOTpqf"
   },
   "source": [
    "# Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5xYA-MlfTpqh"
   },
   "outputs": [],
   "source": [
    "phases = ['train', 'val']\n",
    "config = config['data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "3qTVxBZSTpqn",
    "outputId": "83a66fc0-e25d-42dd-f9c4-22beb17247b1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing datasets\n",
      "Preprocessing with data root dir: /home/apoorv/Documents/Practice/CV/VQA/Visual-Question-Answering/vqadata/\n",
      "Creating tsv datasets: train.tsv, val.tsv\n",
      "TRAIN TSV\n",
      "[('yes', 84978), ('no', 82516), ('1', 12540), ('2', 12215), ('white', 8916), ('3', 6536), ('blue', 5455), ('red', 5201), ('black', 5066), ('0', 4977), ('4', 4118), ('brown', 3814), ('green', 3750), ('yellow', 2792), ('5', 2367), ('gray', 2113), ('nothing', 1814), ('right', 1766), ('frisbee', 1641), ('baseball', 1597), ('left', 1565), ('none', 1563), ('tennis', 1502), ('6', 1455), ('wood', 1449)]\n",
      "Dumping ans-to-idx map to /home/apoorv/Documents/Practice/CV/VQA/Visual-Question-Answering/vqadata/ans_itos.tsv\n",
      "VAL TSV\n",
      "Creating loaders...\n",
      "vocabulary size: 3662\n",
      "Dumping vocabulary to /home/apoorv/Documents/Practice/CV/VQA/Visual-Question-Answering/vqadata/ques_stoi.tsv\n",
      "Dumping train dataset to /home/apoorv/Documents/Practice/CV/VQA/Visual-Question-Answering/vqadata/train.pkl\n",
      "Dumping val dataset to /home/apoorv/Documents/Practice/CV/VQA/Visual-Question-Answering/vqadata/val.pkl\n"
     ]
    }
   ],
   "source": [
    "if preprocess_data:\n",
    "    print('Preprocessing datasets')\n",
    "    preprocess(\n",
    "        data_dir=config['dir'],\n",
    "        train_ques_file=config['train']['ques'],\n",
    "        train_ans_file=config['train']['ans'],\n",
    "            val_ques_file=config['val']['ques'],\n",
    "        val_ans_file=config['val']['ans'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "meCIx1f_Tpqs"
   },
   "outputs": [],
   "source": [
    "datafiles = {x: '{}.pkl'.format(x) for x in phases}\n",
    "img_dir = {x: config[x]['img_dir'] for x in phases}\n",
    "datasets = {x: VQADataset(data_dir=config['dir'], qafile=datafiles[x], img_dir=img_dir[x], phase=x,\n",
    "                          img_scale=config['images']['scale'], img_crop=config['images']['crop'], \n",
    "                          raw_images=True) for x in phases}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gtpdSGU4Tpqw"
   },
   "outputs": [],
   "source": [
    "batch_samplers = {x: VQABatchSampler(datasets[x], config[x]['batch_size']) for x in phases}\n",
    "\n",
    "dataloaders = {x: DataLoader(\n",
    "    datasets[x], batch_sampler=batch_samplers[x], num_workers=config['loader']['workers']) for x in phases}\n",
    "dataset_sizes = {x: len(datasets[x]) for x in phases}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "WZy2WXcDTpq2",
    "outputId": "2a351033-c463-40b0-ba8a-2bd23deb9fe4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': 5000, 'val': 1000}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "undla6alTpq8",
    "outputId": "26d1452b-7605-4f87-d620-aadfd0091552"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 7]) torch.Size([32, 3, 224, 224]) torch.Size([32]) torch.Size([32]) torch.Size([32])\n",
      "157\n"
     ]
    }
   ],
   "source": [
    "for i, d in enumerate(dataloaders[phases[0]]):\n",
    "    print(d[0].shape, d[1].shape, d[2].shape, d[3].shape, d[4].shape)\n",
    "    print(len(dataloaders[phases[0]]))\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_KJanRNJTprE"
   },
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "reCq8bbu93yn"
   },
   "source": [
    "### Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_cgaYqJO99r3"
   },
   "outputs": [],
   "source": [
    "class Attn(nn.Module):\n",
    "    def __init__(self, h_dim):\n",
    "        super(Attn, self).__init__()\n",
    "        self.h_dim = h_dim\n",
    "        self.main = nn.Sequential(\n",
    "            nn.Linear(h_dim, h_dim//2),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(h_dim//2,1)\n",
    "        )\n",
    "\n",
    "    def forward(self, encoder_outputs):\n",
    "        b_size = encoder_outputs.size(0)\n",
    "        attn_ene = self.main(encoder_outputs.contiguous().view(-1, self.h_dim))\n",
    "        return F.softmax(attn_ene.view(b_size, -1), dim=1).unsqueeze(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hvWUS4f_TprF"
   },
   "source": [
    "### Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1vLHLmrITprH"
   },
   "outputs": [],
   "source": [
    "class BlockFusion(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, num_layers, dropout = 0.1, chunks = 16):\n",
    "        super(BlockFusion, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.chunks = chunks\n",
    "        self.dropout = dropout\n",
    "        # ensure that `chunks` wholly divides `output_dim`\n",
    "        self.split_size = int(self.output_dim / self.chunks)\n",
    "        self.size_list = [self.split_size] * self.chunks\n",
    "        self.v_layers = []\n",
    "        self.q_layers = []\n",
    "        for each_size in self.size_list:\n",
    "            v_do = nn.Dropout(p=self.dropout)\n",
    "            v_lin = nn.Linear(each_size, each_size * self.num_layers)\n",
    "            self.v_layers.append(nn.Sequential(v_do, v_lin))\n",
    "\n",
    "            q_do = nn.Dropout(p=self.dropout)\n",
    "            q_lin = nn.Linear(each_size, each_size * self.num_layers)\n",
    "            self.q_layers.append(nn.Sequential(q_do, q_lin))\n",
    "\n",
    "        self.v_layers = nn.ModuleList(self.v_layers)\n",
    "        self.q_layers = nn.ModuleList(self.q_layers)\n",
    "    \n",
    "    def get_chunks(self, embeds, size_list):\n",
    "        out = []\n",
    "        begin = 0\n",
    "        for each_size in size_list:\n",
    "            y = embeds.narrow(1, begin, each_size)\n",
    "            out.append(y)\n",
    "            begin += each_size\n",
    "        return out\n",
    "\n",
    "    def forward(self, ques_emb, img_emb):\n",
    "        batch_size = img_emb.size()[0]\n",
    "        img_emb_chunks = self.get_chunks(img_emb, self.size_list)\n",
    "        ques_emb_chunks = self.get_chunks(ques_emb, self.size_list)\n",
    "        x_mm = []\n",
    "        for i, v_lin, q_lin in zip(range(len(self.size_list)), self.v_layers, self.q_layers):\n",
    "            v_chunk = img_emb_chunks[i]\n",
    "            q_chunk = ques_emb_chunks[i]\n",
    "            m = v_lin(v_chunk) * q_lin(q_chunk)\n",
    "            m = m.view(batch_size, self.num_layers, -1)\n",
    "            z = torch.sum(m, 1)\n",
    "            z = torch.sqrt(F.relu(z)) - torch.sqrt(F.relu(-z))\n",
    "            z = F.normalize(z,p=2)\n",
    "            x_mm.append(z)\n",
    "        out = torch.cat(x_mm, 1)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "b1sR_dQyTprQ"
   },
   "source": [
    "### MUTAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PtnxgPV2TprS"
   },
   "outputs": [],
   "source": [
    "class MutanFusion(nn.Module):\n",
    "    def __init__(self, input_dim, out_dim, num_layers):\n",
    "        super(MutanFusion, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.out_dim = out_dim\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        hv = []\n",
    "        for i in range(self.num_layers):\n",
    "            do = nn.Dropout(p=0.5)\n",
    "            lin = nn.Linear(input_dim, out_dim)\n",
    "\n",
    "            hv.append(nn.Sequential(do, lin, nn.Tanh()))\n",
    "        #\n",
    "        self.image_transformation_layers = nn.ModuleList(hv)\n",
    "        #\n",
    "        hq = []\n",
    "        for i in range(self.num_layers):\n",
    "            do = nn.Dropout(p=0.5)\n",
    "            lin = nn.Linear(input_dim, out_dim)\n",
    "            hq.append(nn.Sequential(do, lin, nn.Tanh()))\n",
    "        #\n",
    "        self.ques_transformation_layers = nn.ModuleList(hq)\n",
    "\n",
    "    def forward(self, ques_emb, img_emb):\n",
    "        # Pdb().set_trace()\n",
    "        batch_size = img_emb.size()[0]\n",
    "        x_mm = []\n",
    "        for i in range(self.num_layers):\n",
    "            x_hv = img_emb\n",
    "            x_hv = self.image_transformation_layers[i](x_hv)\n",
    "\n",
    "            x_hq = ques_emb\n",
    "            x_hq = self.ques_transformation_layers[i](x_hq)\n",
    "            x_mm.append(torch.mul(x_hq, x_hv))\n",
    "        #\n",
    "        x_mm = torch.stack(x_mm, dim=1)\n",
    "        x_mm = x_mm.sum(1).view(batch_size, self.out_dim)\n",
    "        x_mm = F.tanh(x_mm)\n",
    "        return x_mm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dmQ12JysTprV"
   },
   "source": [
    "### Extract Image Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JaGwVXmWTprW"
   },
   "outputs": [],
   "source": [
    "class Normalize(nn.Module):\n",
    "    def __init__(self, p=2):\n",
    "        super(Normalize, self).__init__()\n",
    "        self.p = p\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Pdb().set_trace()\n",
    "        x = x / x.norm(p=self.p, dim=1, keepdim=True)\n",
    "        return x\n",
    "\n",
    "\n",
    "class ImageEmbedding(nn.Module):\n",
    "    def __init__(self, image_channel_type='I', output_size=1024, mode='train',\n",
    "                 extract_features=False, features_dir=None):\n",
    "        super(ImageEmbedding, self).__init__()\n",
    "        self.extractor = torchvision.models.vgg16(pretrained=True)\n",
    "        # freeze feature extractor (VGGNet) parameters\n",
    "        for param in self.extractor.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        extactor_fc_layers = list(self.extractor.classifier.children())[:-1]\n",
    "        if image_channel_type.lower() == 'normi':\n",
    "            extactor_fc_layers.append(Normalize(p=2))\n",
    "        self.extractor.classifier = nn.Sequential(*extactor_fc_layers)\n",
    "\n",
    "        self.fflayer = nn.Sequential(\n",
    "            nn.Linear(4096, output_size),\n",
    "            nn.Tanh())\n",
    "\n",
    "        # TODO: Get rid of this hack\n",
    "        self.mode = mode\n",
    "        self.extract_features = extract_features\n",
    "        self.features_dir = features_dir\n",
    "\n",
    "    def forward(self, image, image_ids):\n",
    "        # Pdb().set_trace()\n",
    "        if self.extract_features:\n",
    "            image = self.extractor(image)\n",
    "            #if self.features_dir is not None:\n",
    "                #utils.save_image_features(image, image_ids, self.features_dir)\n",
    "        image_embedding = self.fflayer(image)\n",
    "        return image_embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "N0IQPiQjTprb"
   },
   "source": [
    "### Question Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3tGiDXbUTprc"
   },
   "outputs": [],
   "source": [
    "class QuesEmbedding(nn.Module):\n",
    "    def __init__(self, input_size=300, hidden_size=512, output_size=1024, num_layers=2, batch_first=True):\n",
    "        super(QuesEmbedding, self).__init__()\n",
    "        # TODO: take as parameter\n",
    "        self.bidirectional = True\n",
    "        if num_layers == 1:\n",
    "            self.lstm = nn.LSTM(input_size=input_size, hidden_size=hidden_size,\n",
    "                                batch_first=batch_first, bidirectional=self.bidirectional)\n",
    "\n",
    "            if self.bidirectional:\n",
    "                self.fflayer = nn.Sequential(\n",
    "                    nn.Linear(2 * num_layers * hidden_size, output_size),\n",
    "                    nn.Tanh())\n",
    "        else:\n",
    "            self.lstm = nn.LSTM(input_size=input_size, hidden_size=hidden_size,\n",
    "                                num_layers=num_layers, batch_first=batch_first)\n",
    "            self.fflayer = nn.Sequential(\n",
    "                nn.Linear(2 * num_layers * hidden_size, output_size),\n",
    "                nn.Tanh())\n",
    "        if use_att:\n",
    "            self.attn = Attn(1024)\n",
    "\n",
    "    def forward(self, ques):\n",
    "        output, hx = self.lstm(ques)\n",
    "        if use_att:\n",
    "            output = output.permute(1,0,2)\n",
    "            attention_weights = self.attn(output)\n",
    "            ques_embedding = (output * attention_weights).sum(dim=1)\n",
    "        else:\n",
    "            lstm_embedding = torch.cat([hx[0], hx[1]], dim=2)\n",
    "            ques_embedding = lstm_embedding[0]\n",
    "            if self.lstm.num_layers > 1 or self.bidirectional:\n",
    "                for i in range(1, self.lstm.num_layers):\n",
    "                    ques_embedding = torch.cat(\n",
    "                        [ques_embedding, lstm_embedding[i]], dim=1)\n",
    "                ques_embedding = self.fflayer(ques_embedding)\n",
    "        return ques_embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MGNoxQfiTprg"
   },
   "source": [
    "### Complete Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hAlkrFnBTpri"
   },
   "outputs": [],
   "source": [
    "class VQAModel(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size=10000, word_emb_size=300, emb_size=1024, output_size=1000, image_channel_type='I', ques_channel_type='lstm', use_mutan=True, mode='train', extract_img_features=True, features_dir=None):\n",
    "        super(VQAModel, self).__init__()\n",
    "        self.mode = mode\n",
    "        self.word_emb_size = word_emb_size\n",
    "        self.image_channel = ImageEmbedding(image_channel_type, output_size=emb_size, mode=mode,\n",
    "                                            extract_features=extract_img_features, features_dir=features_dir)\n",
    "\n",
    "        # NOTE the padding_idx below.\n",
    "        self.word_embeddings = nn.Embedding(vocab_size, word_emb_size)\n",
    "        if ques_channel_type.lower() == 'lstm':\n",
    "            self.ques_channel = QuesEmbedding(\n",
    "                input_size=word_emb_size, output_size=emb_size, num_layers=1, batch_first=False)\n",
    "        elif ques_channel_type.lower() == 'deeplstm':\n",
    "            self.ques_channel = QuesEmbedding(\n",
    "                input_size=word_emb_size, output_size=emb_size, num_layers=2, batch_first=False)\n",
    "        else:\n",
    "            msg = 'ques channel type not specified. please choose one of -  lstm or deeplstm'\n",
    "            print(msg)\n",
    "            raise Exception(msg)\n",
    "        if 'mutan' in fusion:\n",
    "            self.mutan = MutanFusion(emb_size, emb_size, 5)\n",
    "            self.mlp = nn.Sequential(nn.Linear(emb_size, output_size))\n",
    "        elif 'block' in fusion:\n",
    "            self.block = BlockFusion(emb_size, emb_size, 5)\n",
    "            self.mlp = nn.Sequential(nn.Linear(emb_size, output_size))\n",
    "        else:\n",
    "            self.mlp = nn.Sequential(\n",
    "                nn.Linear(emb_size, 1000),\n",
    "                nn.Dropout(p=0.5),\n",
    "                nn.Tanh(),\n",
    "                nn.Linear(1000, output_size))\n",
    "\n",
    "    def forward(self, images, questions, image_ids):\n",
    "        image_embeddings = self.image_channel(images, image_ids)\n",
    "        embeds = self.word_embeddings(questions)\n",
    "        ques_embeddings = self.ques_channel(embeds)\n",
    "        if 'mutan' in fusion:\n",
    "            combined = self.mutan(ques_embeddings, image_embeddings)\n",
    "        elif 'block' in fusion:\n",
    "            combined = self.block(ques_embeddings, image_embeddings)\n",
    "        else:\n",
    "            combined = image_embeddings * ques_embeddings\n",
    "        output = self.mlp(combined)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IQzyT2okTprl"
   },
   "outputs": [],
   "source": [
    "model = VQAModel(vocab_size=len(VQADataset.ques_vocab), output_size=len(VQADataset.ans_vocab)).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JHemD9loTprp"
   },
   "source": [
    "# Training Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rzNDIBI6Tprq"
   },
   "outputs": [],
   "source": [
    "ROOT_DIR = './vqa_results/'\n",
    "now = fusion\n",
    "\n",
    "if not os.path.exists(ROOT_DIR):\n",
    "    os.makedirs(ROOT_DIR)\n",
    "\n",
    "if not os.path.exists(ROOT_DIR + now):\n",
    "    os.makedirs(ROOT_DIR + now)\n",
    "\n",
    "LOG_DIR = ROOT_DIR + now + '/logs/'\n",
    "if not os.path.exists(LOG_DIR):\n",
    "    os.makedirs(LOG_DIR)\n",
    "\n",
    "OUTPUTS_DIR = ROOT_DIR  + now + '/outputs/'\n",
    "if not os.path.exists(OUTPUTS_DIR):\n",
    "    os.makedirs(OUTPUTS_DIR)\n",
    "\n",
    "MODEL_DIR = ROOT_DIR + now + '/models/'\n",
    "if not os.path.exists(MODEL_DIR):\n",
    "    os.makedirs(MODEL_DIR)\n",
    "\n",
    "summary_writer = SummaryWriter(LOG_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ed50h3GpTpru"
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(filter(lambda p: p.requires_grad, model.parameters()), momentum=0.9, \n",
    "                           lr=0.001, weight_decay=5.0e-4)\n",
    "scheduler_optimizer = torch.optim.lr_scheduler.MultiStepLR(optimizer, \n",
    "                                                        milestones=[50,75], gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kSPU9iA6Tprx"
   },
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zdxpg_CwTpr2"
   },
   "outputs": [],
   "source": [
    "max_epoch = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "fNxwrgAJTpr6",
    "outputId": "12ba9137-9519-4f2b-9ee0-6a916a3a42f1",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for epoch in range(max_epoch):\n",
    "    model.train()\n",
    "    for i, d in enumerate(dataloaders['train']):\n",
    "        questions, images, image_ids, answers, ques_ids = d[0], d[1], d[2], d[3], d[4]\n",
    "        images = images.to(device)\n",
    "        questions = questions.permute(1,0).to(device)\n",
    "        answers = answers.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        ans_scores = model(images, questions, image_ids)\n",
    "        _, preds = torch.max(ans_scores, 1)\n",
    "        loss = criterion(ans_scores, answers)\n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        running_corrects = torch.sum((preds == answers).data)\n",
    "        \n",
    "        print('Epoch: {}, Iteration: {}/{}'.format(epoch, i, len(dataloaders['train'])))\n",
    "        print('Loss: {}'.format(loss.item()))\n",
    "        print('Correct Answers: {}'.format(running_corrects.item()))\n",
    "        print('\\n')\n",
    "        \n",
    "        summary_writer.add_scalar(\"Training Loss\", loss.item())\n",
    "        summary_writer.add_scalar(\"Training Running Correct\", running_corrects.item())\n",
    "        \n",
    "    model.eval()\n",
    "    for i, d in enumerate(dataloaders['val']):\n",
    "        questions, images, image_ids, answers, ques_ids = d[0], d[1], d[2], d[3], d[4]\n",
    "        images = images.to(device)\n",
    "        questions = questions.permute(1,0).to(device)\n",
    "        answers = answers.to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            ans_scores = model(images, questions, image_ids)\n",
    "            _, preds = torch.max(ans_scores, 1)\n",
    "            loss = criterion(ans_scores, answers)\n",
    "        \n",
    "        running_corrects = torch.sum((preds == answers).data)\n",
    "        \n",
    "        print('Epoch: {}, Iteration: {}/{}'.format(epoch, i, len(dataloaders['val'])))\n",
    "        print('Validation Loss: {}'.format(loss.item()))\n",
    "        print('Validation Correct Answers: {}'.format(running_corrects.item()))\n",
    "        print('\\n')\n",
    "        \n",
    "        summary_writer.add_scalar(\"Validation Loss\", loss.item())\n",
    "        summary_writer.add_scalar(\"Validation Running Correct\", running_corrects.item())\n",
    "    \n",
    "    torch.save(model.state_dict(), MODEL_DIR+'vqa_{}_{}.pth'.format(epoch, now))\n",
    "    scheduler_optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UqJGtwXSfHZL"
   },
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VQAModel(vocab_size=len(VQADataset.ques_vocab), output_size=len(VQADataset.ans_vocab)).to(device)\n",
    "model.load_state_dict(torch.load(path_to_weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DSMdgPLXwGAO"
   },
   "outputs": [],
   "source": [
    "def evaluate(mode='val'):\n",
    "    model.eval()\n",
    "    total_corrects = 0.0\n",
    "    for i, d in enumerate(dataloaders[mode]):\n",
    "        questions, images, image_ids, answers, ques_ids = d[0], d[1], d[2], d[3], d[4]\n",
    "        images = images.to(device)\n",
    "        questions = questions.permute(1,0).to(device)\n",
    "        answers = answers.to(device)\n",
    "            \n",
    "        with torch.no_grad():\n",
    "            ans_scores = model(images, questions, image_ids)\n",
    "            _, preds = torch.max(ans_scores, 1)\n",
    "            loss = criterion(ans_scores, answers)\n",
    "            \n",
    "        running_corrects = torch.sum((preds == answers).data)\n",
    "        total_corrects += running_corrects\n",
    "            \n",
    "        acc = total_corrects / len(datasets[mode])\n",
    "    print('Epoch: {}, Accuracy: {}'.format(epoch, acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Madhur_VQA_Fusion.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
